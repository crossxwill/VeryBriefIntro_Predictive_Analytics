{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Modeling with `statsmodels`\n",
    "---\n",
    "\n",
    "As we've seen, `scikit-learn` is an essential package but geared towards machine learning rather than statistics.  The `statsmodels` package brings more traditional statistics functionality to Python.  The two can be used together in various ways, so knowing both is handy for complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "---\n",
    "\n",
    "Let's re-visit the Boston housing dataset, loading it via `load_boston()`, creating a data frame of features and a series of labels, then combining them together with `concat()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "\n",
    "features_df = pd.DataFrame(boston['data'], columns = boston['feature_names'])\n",
    "labels = pd.Series(boston['target'], name = 'MEDV')\n",
    "\n",
    "training_df = pd.concat([features_df, labels], axis = 1)\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `statsmodels` package has a different API than `scikit-learn` and so takes some getting used to.  Let's create an OLS (ordinary linear regression) class from a formula that specifies the dependent variable on the left of a tilde (`~`) with the independent variables on the right, separated by plus signs.  We also tell it which data frame to reference those names in via the `data` parameter.\n",
    "\n",
    "After creating a model, we call `fit()` but don't need to pass any parameters here.  Finally, we call `summary()` on the fit object to get R-like output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(formula = 'MEDV ~ RM + RAD', data = training_df)\n",
    "fit = model.fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems comparable to before -- let's see how it performs using all predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(formula = 'MEDV ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT', data = training_df)\n",
    "fit = model.fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also seems comparable.  In R, there is a nice shortcut enabled by formulas where listing a period to the right of a tilde will use all columns as predictors that aren't on the left.  Due to limitations in `statsmodels`, we can't do that, so I provided a function here that will generate such a formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_formula(variables, dependent_variable_name):\n",
    "    independent_variables = set(variables) - set([dependent_variable_name])\n",
    "    return dependent_variable_name + ' ~ ' + ' + '.join(independent_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = get_formula(training_df.columns, 'MEDV')\n",
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(formula = formula, data = training_df)\n",
    "fit = model.fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Diagnostics\n",
    "---\n",
    "\n",
    "There are various diagnostic plots for checking the assumptions of the model. One of these, `plot_regress_exog()`, plots a 2x2 grid of figures showing the true and predicted labels vs the independent variable, residuals vs the independent variable, a partial regression plot, and a component-component plus residual (CCPR) plot.\n",
    "\n",
    "See the official [`statsmodels` notebooks](http://www.statsmodels.org/stable/examples/notebooks/generated/regression_plots.html) for more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.graphics.plot_regress_exog(fit, 'RM', fig = plt.figure(figsize = (12, 12)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "---\n",
    "\n",
    "We can perform classification, where the dependent variable is binary rather than continuous, using *logistic regression*.  A common dataset for demonstrating logistic regression is the breast cancer dataset describing malignant and benign tumors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = load_breast_cancer()\n",
    "\n",
    "safe_columns = [_.replace(' ', '_') for _ in bc['feature_names']]\n",
    "features_df = pd.DataFrame(bc['data'], columns = safe_columns)\n",
    "labels = pd.Series(bc['target'], name = 'malignant')\n",
    "\n",
    "features_df = features_df.loc[:, ~features_df.columns.str.startswith('worst')]\n",
    "\n",
    "training_df = pd.concat([features_df, labels], axis = 1)\n",
    "training_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity we've dropped a set of features beginning with the word `worst`.\n",
    "\n",
    "Our code is the same excelt we call the `glm()` function, and set the `family` parameter in order to apply the logit transformation necessary for binary classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.glm(\n",
    "    formula = get_formula(training_df.columns, 'malignant'),\n",
    "    data = training_df,\n",
    "    family = sm.families.Binomial()\n",
    ")\n",
    "fit = model.fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with `scikit-learn`\n",
    "---\n",
    "\n",
    "It's quite simple to apply logistic regression using `scikit-learn` instead.  We simply use the `LogisticRegression()` class rather than `LinearRegression()`, and we use different performance metrics such as accuracy, precision, or recall to evaluate the model.\n",
    "\n",
    "Avoid using accuracy whenever possible, due to issues discussed in depth in advanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import *\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(features_df, labels)\n",
    "predictions = model.predict(features_df)\n",
    "\n",
    "print(accuracy_score(labels, predictions))\n",
    "print(precision_score(labels, predictions))\n",
    "print(recall_score(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.Series(model.coef_[0], index = features_df.columns).sort_values()\n",
    "print(coefs.head())\n",
    "print()\n",
    "print(coefs.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the coefficients don't match those from R: `scikit-learn` uses different methods for the optimization problem that learns the coefficients, so the performance will likely be similar but the coefficients may not be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling R from Python\n",
    "---\n",
    "\n",
    "Finally, we can compare our output directly to R from within Python.  In fact, using the `rpy2` package, we can call any R function and convert back and forth between Python and R data types.\n",
    "\n",
    "We first load the `broom` package in R to convert the output of `glm` into a data frame so the results returned to Python are easily readable.  We then call the `pandas2ri.activate()` function which automatically translates Pandas data frames to R data frames and back.  Finally, we fit a logistic regression model (`glm()` with `family = 'binomial'`) and print the summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpy2.robjects import r, pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "importr('broom')\n",
    "\n",
    "pandas2ri.activate()\n",
    "\n",
    "fit = r.glm(\n",
    "    'malignant ~ .',\n",
    "    data = training_df,\n",
    "    family = 'binomial'\n",
    ")\n",
    "print(r.tidy(fit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike `scikit-learn`, these coefficients should match our `statsmodels` output above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
